# -*- coding: utf-8 -*-
"""05-Module4-HW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ueMdXmteuhkQqUfmbIv33OLNRwMMcOf4

### Exercise 1

1) Load the breast_cancer dataset from skelarn (from sklearn.datasets import load_breast_cancer). Split the dataset into training and test datasets. Scale the dataset using minmaxscaler. Use KNeighborsClassifier classifier and report the score on the test dataset.
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier

#Loading the data
data = load_breast_cancer()
X,y= data.data, data.target
#split into traioning and test
X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size=0.2, random_state = 42)

# Scaling the dataset using minmaxscaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Applying KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors= 5)
knn.fit(X_train_scaled, y_train)

score = knn.score(X_test_scaled, y_test)
score

"""2) Repeat Step 1 using pipelines and report the score."""

from sklearn.pipeline import Pipeline

pipeline = Pipeline([('scaler', MinMaxScaler()), # step 1 scaling
                     ('knn', KNeighborsClassifier(n_neighbors = 5))]) #step2 KNN classifier

#train the pipeline
pipeline.fit(X_train, y_train)

#evaluate the model
score = pipeline.score(X_test, y_test)
score

"""3) Use the pipeline object from Step 2 and make a grid search on parameter of number of neighbor."""

from sklearn.model_selection import GridSearchCV

#define parameter grid for gridserachCV
param_grid= {'knn__n_neighbors': [3,5,7,9,11]} # testing different k values

grid_search = GridSearchCV(pipeline, param_grid, cv= 5, scoring = 'accuracy', n_jobs = -1)
grid_search.fit(X_train, y_train)

# Print the best parameters and corresponding accuracy
print(f'Best n_neighbors: {grid_search.best_params_["knn__n_neighbors"]}')
print(f'Best cross-validation accuracy: {grid_search.best_score_:.4f}')

best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)

print(f'Best model test accuracy: {test_score:.4f}')

"""4)	Import bike_day_raw.csv. Create a pipeline using ColumnTransformer, Scaling, and KNeighborsRegressor.

- Use `from sklearn.neighbors import KNeighborsRegressor'
- You need to split the data into X and y.
- Check the data shape
- Check the data types
- Print the column names of the data frame
- Create a scatterplot of each feature against the target variable
- Create alist of features that are numeric and not numeric
- Create a pipeline of imputer and standard scaler for the numeric features
- Create a column transformer which uses the pipeline you created for numeric features and a onehotencoder for the non-numeric features
- You can create your column transformer in different ways
- Finally create a pipeline of column transformer and kNeighborsRegressor
- Split your data into train and test datasets
- Report the score on the test dataset


"""

import pandas as pd

df = pd.read_csv('bike_day_raw.csv')

df.shape

df.dtypes

df.columns

X = df.drop(columns=['cnt'])
y = df['cnt']

import matplotlib.pyplot as plt

# Create scatter plots of each feature against the target variable
for col in X.columns:
  plt.figure(figsize=(6,4))
  plt.scatter(X[col],y, alpha=0.5)
  plt.xlabel(col)
  plt.ylabel('cnt')
  plt.title(f'Scatter plot of {col} against cnt')
  plt.show()

# Identify numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
numeric_features

categorical_features = X.select_dtypes(include=['object']).columns
categorical_features

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Create a pipeline for numeric features (Imputation + Scaling)
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')), # filling missing values
    ('scaler', StandardScaler()) # standard scaling
])

# Create a transformer for categorical features (OneHotEncoding)
from sklearn.preprocessing import OneHotEncoder
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

from sklearn.compose import ColumnTransformer
from sklearn.neighbors import KNeighborsRegressor

# ColumnTransformer combining both numeric and categorical transformations
preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features)
])

# Create final pipeline with preprocessor and KNeighborsRegressor
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('knn', KNeighborsRegressor()) # here default n_neighbors=5
])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the pipeline
pipeline.fit(X_train, y_train)

# Report the score on the test dataset
test_score = pipeline.score(X_test, y_test)
print(f"\nKNeighborsRegressor Test Score: {test_score:.4f}")